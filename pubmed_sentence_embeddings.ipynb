{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d411a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow tqdm nltk sentence-transformers huggingface_hub pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e218c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import yaml\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the PMIDs from the file\n",
    "pmid_file_path = './pmids.txt'\n",
    "with open(pmid_file_path, 'r') as f:\n",
    "    pmids = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch a PubMed article in XML format by its PMID\n",
    "def fetch_pubmed_article(pmid):\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid}&retmode=xml\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch articles and process them\n",
    "pmid_list = []\n",
    "sentence_list = []\n",
    "embedding_list = []\n",
    "\n",
    "for pmid in tqdm(pmids, desc=\"Downloading PubMed Articles\"):\n",
    "    article_xml = fetch_pubmed_article(pmid)\n",
    "    if article_xml:\n",
    "        # Parse the XML to extract the abstract\n",
    "        root = ET.fromstring(article_xml)\n",
    "        abstract_texts = root.findall(\".//AbstractText\")\n",
    "        abstract = \" \".join(abstract_text.text for abstract_text in abstract_texts if abstract_text is not None)\n",
    "        \n",
    "        # Tokenize the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        \n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Store the PMIDs, sentences, and embeddings\n",
    "        pmid_list.extend([pmid] * len(sentences))\n",
    "        sentence_list.extend(sentences)\n",
    "        embedding_list.extend(embeddings)\n",
    "\n",
    "# Create a DataFrame with PMIDs, sentences, and embeddings\n",
    "data = {'PMID': pmid_list, 'sentence': sentence_list, 'embedding': embedding_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to Parquet format\n",
    "parquet_file_path = './data/sentence_embeddings.parquet'\n",
    "df.to_parquet(parquet_file_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Embeddings saved to {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metadata in venomx format\n",
    "metadata = {\n",
    "    'id': 'https://w3id.org/biolink/biolinkml/venomx',\n",
    "    'name': 'PubMed Sentences',\n",
    "    'title': 'PubMed Sentence Embeddings',\n",
    "    'description': 'Embeddings for sentences extracted from a set of PubMed abstracts of interest. These are not embeddings of all of Pubmed. There is a separate embedding for each sentence in each abstract.',\n",
    "    'license': 'https://opensource.org/licenses/BSD-3-Clause',\n",
    "    'prefixes': {\n",
    "        'venomx': 'https://w3id.org/biolink/biolinkml/venomx/',\n",
    "        'linkml': 'https://w3id.org/linkml/'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save the metadata to a YAML file\n",
    "metadata_file_path = './data/metadata.yaml'\n",
    "with open(metadata_file_path, 'w') as f:\n",
    "    yaml.dump(metadata, f)\n",
    "\n",
    "print(f\"Metadata saved to {metadata_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca37dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face\n",
    "repo_id = \"biomedical-translator/pubmed_sentence_embeddings\"\n",
    "this_notebook = \"pubmed_sentence_embeddings.ipynb\"\n",
    "create_repo(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "api = HfApi()\n",
    "files_to_upload = [parquet_file_path, metadata_file_path, this_notebook]\n",
    "\n",
    "for file in files_to_upload:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file,\n",
    "        path_in_repo=file,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "\n",
    "print(f\"Files uploaded to Hugging Face in repository: {repo_id}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
