{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow tqdm nltk sentence-transformers huggingface_hub pyyaml\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c4973586542f9f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import yaml\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "829144ba3dfd450a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the PMIDs from the file\n",
    "pmid_file_path = './pmids.txt'\n",
    "with open(pmid_file_path, 'r') as f:\n",
    "    pmids = f.read().splitlines()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "363e4cd840e6d77e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to fetch a PubMed article in XML format by its PMID\n",
    "def fetch_pubmed_article(pmid):\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid}&retmode=xml\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        return None\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82ba593571b15599"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82bcb60f882f9c64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fetch articles and process them\n",
    "pmid_list = []\n",
    "sentence_list = []\n",
    "embedding_list = []\n",
    "\n",
    "for pmid in tqdm(pmids, desc=\"Downloading PubMed Articles\"):\n",
    "    article_xml = fetch_pubmed_article(pmid)\n",
    "    if article_xml:\n",
    "        # Parse the XML to extract the abstract\n",
    "        root = ET.fromstring(article_xml)\n",
    "        abstract_texts = root.findall(\".//AbstractText\")\n",
    "        try:\n",
    "            abstract = \" \".join(abstract_text.text for abstract_text in abstract_texts if abstract_text.text is not None)\n",
    "        except TypeError:\n",
    "            import warnings\n",
    "            warnings.warn(f\"PMID {pmid} has no abstract\")\n",
    "            continue\n",
    "        \n",
    "        if not abstract.strip():\n",
    "            import warnings\n",
    "            warnings.warn(f\"PMID {pmid} has an empty abstract\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        \n",
    "        if not sentences:\n",
    "            import warnings\n",
    "            warnings.warn(f\"PMID {pmid} resulted in empty sentences after tokenization\")\n",
    "            continue\n",
    "\n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Store the PMIDs, sentences, and embeddings\n",
    "        pmid_list.extend([pmid] * len(sentences))\n",
    "        sentence_list.extend(sentences)\n",
    "        embedding_list.extend(embeddings)\n",
    "\n",
    "# Create a DataFrame with PMIDs, sentences, and embeddings\n",
    "data = {'PMID': pmid_list, 'sentence': sentence_list, 'embedding': embedding_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to Parquet format\n",
    "parquet_file_path = './data/sentence_embeddings.parquet'\n",
    "df.to_parquet(parquet_file_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Embeddings saved to {parquet_file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7802adc94111daf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate metadata in venomx format\n",
    "metadata = {\n",
    "    'description': 'Embeddings of each sentence in the abstracts of about 6000 selected PubMed articles. Github repo with jupyter notebook and data files is here: https://github.com/justaddcoffee/embed_pubmed',\n",
    "    'prefixes': {\n",
    "        'PMID': 'https://pubmed.ncbi.nlm.nih.gov/PMID_'\n",
    "    },\n",
    "    'model': {\n",
    "        'name': 'all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'Pubmed July 2024',\n",
    "        'url': 'https://huggingface.co/biomedical-translator/pubmed_sentence_embeddings'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the metadata to a YAML file\n",
    "metadata_file_path = './data/metadata.yaml'\n",
    "with open(metadata_file_path, 'w') as f:\n",
    "    yaml.dump(metadata, f)\n",
    "\n",
    "print(f\"Metadata saved to {metadata_file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eea4fe2a4bdae560"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca37dc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:54:20.431353Z",
     "start_time": "2024-07-30T16:54:20.430552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upload to Hugging Face\n",
    "repo_id = \"biomedical-translator/pubmed_sentence_embeddings\"\n",
    "create_repo(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "this_notebook_path = \"pubmed_sentence_embeddings.ipynb\"\n",
    "\n",
    "api = HfApi()\n",
    "files_to_upload = [parquet_file_path, metadata_file_path, this_notebook_path]\n",
    "\n",
    "for file in files_to_upload:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file,\n",
    "        path_in_repo=file,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "\n",
    "print(f\"Files uploaded to Hugging Face in repository: {repo_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
