{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.11/site-packages (17.0.0)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.11/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.11/site-packages (0.24.3)\r\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (6.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.11/site-packages (from nltk) (2024.7.24)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (4.43.3)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (2.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (1.5.1)\r\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (1.14.0)\r\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (3.15.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2024.6.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (24.1)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.7.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow tqdm nltk sentence-transformers huggingface_hub pyyaml\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T17:30:02.287615Z",
     "start_time": "2024-07-30T17:29:59.759060Z"
    }
   },
   "id": "7c4973586542f9f0"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jtr4v/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import yaml\n",
    "import nltk\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T17:30:02.408371Z",
     "start_time": "2024-07-30T17:30:02.291039Z"
    }
   },
   "id": "829144ba3dfd450a"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Read the PMIDs from the file\n",
    "pmid_file_path = './pmids.txt'\n",
    "with open(pmid_file_path, 'r') as f:\n",
    "    pmids = f.read().splitlines()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T17:30:02.412528Z",
     "start_time": "2024-07-30T17:30:02.405680Z"
    }
   },
   "id": "363e4cd840e6d77e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Function to fetch a PubMed article in XML format by its PMID\n",
    "def fetch_pubmed_article(pmid):\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid}&retmode=xml\"\n",
    "    response = requests.get(url, headers={\"Accept-Encoding\": \"identity\"})\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T17:30:02.412760Z",
     "start_time": "2024-07-30T17:30:02.410489Z"
    }
   },
   "id": "82ba593571b15599"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T17:30:03.403884Z",
     "start_time": "2024-07-30T17:30:02.413859Z"
    }
   },
   "id": "82bcb60f882f9c64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PubMed Articles:   1%|          | 55/6593 [00:17<40:57,  2.66it/s] "
     ]
    }
   ],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Fetch articles and process them\n",
    "pmid_list = []\n",
    "sentence_list = []\n",
    "embedding_list = []\n",
    "\n",
    "for pmid in tqdm(pmids, desc=\"Downloading PubMed Articles\"):\n",
    "    article_xml = fetch_pubmed_article(pmid)\n",
    "    if article_xml:\n",
    "        # Parse the XML to extract the abstract\n",
    "        root = ET.fromstring(article_xml)\n",
    "        abstract_texts = root.findall(\".//AbstractText\")\n",
    "        try:\n",
    "            abstract = \" \".join(abstract_text.text for abstract_text in abstract_texts if abstract_text.text is not None)\n",
    "        except TypeError:\n",
    "            warnings.warn(f\"PMID {pmid} has no abstract\")\n",
    "            continue\n",
    "        \n",
    "        if not abstract.strip():\n",
    "            warnings.warn(f\"PMID {pmid} has an empty abstract\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize the abstract into sentences\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        \n",
    "        if not sentences:\n",
    "            warnings.warn(f\"PMID {pmid} resulted in empty sentences after tokenization\")\n",
    "            continue\n",
    "\n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Store the PMIDs, sentences, and embeddings\n",
    "        pmid_list.extend([pmid] * len(sentences))\n",
    "        sentence_list.extend(sentences)\n",
    "        embedding_list.extend(embeddings)\n",
    "\n",
    "# Create a DataFrame with PMIDs, sentences, and embeddings\n",
    "data = {'PMID': pmid_list, 'sentence': sentence_list, 'embedding': embedding_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to Parquet format\n",
    "parquet_file_path = './data/sentence_embeddings.parquet'\n",
    "df.to_parquet(parquet_file_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Embeddings saved to {parquet_file_path}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-30T17:30:03.407508Z"
    }
   },
   "id": "7802adc94111daf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate metadata in venomx format\n",
    "metadata = {\n",
    "    'description': 'Embeddings of each sentence in the abstracts of about 6000 selected PubMed articles. Github repo with jupyter notebook and data files is here: https://github.com/justaddcoffee/embed_pubmed',\n",
    "    'prefixes': {\n",
    "        'PMID': 'https://pubmed.ncbi.nlm.nih.gov/PMID_'\n",
    "    },\n",
    "    'model': {\n",
    "        'name': 'all-MiniLM-L6-v2'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'name': 'Pubmed July 2024',\n",
    "        'url': 'https://huggingface.co/biomedical-translator/pubmed_sentence_embeddings'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the metadata to a YAML file\n",
    "metadata_file_path = './data/metadata.yaml'\n",
    "with open(metadata_file_path, 'w') as f:\n",
    "    yaml.dump(metadata, f)\n",
    "\n",
    "print(f\"Metadata saved to {metadata_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eea4fe2a4bdae560"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca37dc6",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Upload to Hugging Face\n",
    "repo_id = \"biomedical-translator/pubmed_sentence_embeddings\"\n",
    "create_repo(repo_id, repo_type=\"dataset\")\n",
    "\n",
    "this_notebook_path = \"pubmed_sentence_embeddings.ipynb\"\n",
    "\n",
    "api = HfApi()\n",
    "files_to_upload = [parquet_file_path, metadata_file_path, this_notebook_path]\n",
    "\n",
    "for file in files_to_upload:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=file,\n",
    "        path_in_repo=file,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "\n",
    "print(f\"Files uploaded to Hugging Face in repository: {repo_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
